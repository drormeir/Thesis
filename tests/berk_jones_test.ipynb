{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    # import glob; print(glob.glob('/content/gdrive/Othercomputers/My Laptop/projects/RUNI/Thesis/*'))\n",
    "    sys.path.append('/content/gdrive/Othercomputers/My Laptop/projects/RUNI/Thesis')\n",
    "except:\n",
    "    import os\n",
    "    currentdir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    sys.path.append(currentdir) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta, chi2\n",
    "import numpy as np\n",
    "\n",
    "MIN_PVAL = 1e-20\n",
    "\n",
    "\n",
    "class MultiTest(object):\n",
    "    \"\"\"\n",
    "    Higher Criticism test \n",
    "\n",
    "    References:\n",
    "    [1] Donoho, D. L. and Jin, J.,\n",
    "     \"Higher criticism for detecting sparse hetrogenous mixtures\", \n",
    "     Annals of Stat. 2004\n",
    "    [2] Donoho, D. L. and Jin, J. \"Higher critcism thresholding: Optimal \n",
    "    feature selection when useful features are rare and weak\", proceedings\n",
    "    of the national academy of sciences, 2008.\n",
    "    ========================================================================\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        pvals    list of p-values. P-values that are np.nan are exluded.\n",
    "        stbl     normalize by expected P-values (stbl=True) or observed\n",
    "                 P-values (stbl=False). stbl=True was suggested in [2].\n",
    "                 stbl=False in [1].\n",
    "        gamma    lower fruction of p-values to use.\n",
    "        \n",
    "    Methods :\n",
    "    -------\n",
    "        hc       HC and P-value attaining it\n",
    "        hc_star   more stable version of HC (HCdagger in [1])\n",
    "        hc_jin    a version of HC from \n",
    "                [2] Jiashun Jin and Wanjie Wang, \"Influential features PCA for\n",
    "                 high dimensional clustering\"\n",
    "\n",
    "    Todo:\n",
    "      Implement Feature selection procedures: HC-thresholding, FDR, BJ, Sims\n",
    "      The idea is to return a mask based on the P-values\n",
    "      Perhaps implement it in a different module dedicated to feature selection?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pvals, stbl=True):\n",
    "\n",
    "        self._N = len(pvals)\n",
    "        assert (self._N > 0)\n",
    "\n",
    "        EPS = 1 / self._N\n",
    "        self._EPS = EPS\n",
    "        self._istar = 1\n",
    "\n",
    "        self._pvals = np.sort(np.asarray(pvals.copy()))\n",
    "        self._uu = np.linspace(1 / self._N, 1 - EPS, self._N)\n",
    "\n",
    "        if stbl:\n",
    "            denom = np.sqrt(self._uu * (1 - self._uu))\n",
    "        else:\n",
    "            denom = np.sqrt(self._pvals * (1 - self._pvals))\n",
    "\n",
    "        denom = np.maximum(denom, EPS)\n",
    "        self._zz = np.sqrt(self._N) * (self._uu - self._pvals) / denom\n",
    "\n",
    "        self._imin_star = np.argmax(self._pvals > (1 - self._EPS) / self._N)\n",
    "        self._imin_jin = np.argmax(self._pvals > np.log(self._N) / self._N)\n",
    "\n",
    "    def _calculate_hc(self, imin, imax):\n",
    "        if imin > imax:\n",
    "            return np.nan\n",
    "        if imin == imax:\n",
    "            self._istar = imin\n",
    "        else:\n",
    "            self._istar = np.argmax(self._zz[imin:imax]) + imin\n",
    "        zMaxStar = self._zz[self._istar]\n",
    "        return zMaxStar, self._pvals[self._istar]\n",
    "\n",
    "    def hc(self, gamma=0.2):\n",
    "        \"\"\"\n",
    "        Higher Criticism test statistic\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        gamma   lower fraction of P-values to consider\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        HC test score, P-value attaining it\n",
    "\n",
    "        \"\"\"\n",
    "        imin = 0\n",
    "        imax = np.maximum(imin, int(gamma * self._N + 0.5))\n",
    "        return self._calculate_hc(imin, imax)\n",
    "\n",
    "    def hc_jin(self, gamma=0.2):\n",
    "        \"\"\"sample-adjusted higher criticism score from [2]\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        gamma   lower fraction of P-values to consider\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        HC score, P-value attaining it\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        imin = self._imin_jin\n",
    "        imax = np.maximum(imin + 1, int(np.floor(gamma * self._N + 0.5)))\n",
    "        return self._calculate_hc(imin, imax)\n",
    "\n",
    "    def berk_jones(self, gamma=.45):\n",
    "        \"\"\"\n",
    "        Exact Berk-Jones statistic\n",
    "\n",
    "        According to Moscovich, Nadler, Spiegelman. (2013). \n",
    "        On the exact Berk-Jones statistics and their p-value calculation\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        gamma  lower fraction of P-values to consider. Better to pick\n",
    "               gamma < .5 or far below 1 to avoid p-values that are one\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        -log(BJ) score (large values are significant) \n",
    "        (has a scaled chisquared distribution under the null)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        N = self._N\n",
    "\n",
    "        if N == 0:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        max_i = max(1, int(gamma * N))\n",
    "\n",
    "        spv = self._pvals[:max_i]\n",
    "        ii = np.arange(1, max_i + 1)\n",
    "\n",
    "        bj = spv[0]\n",
    "        if len(spv) >= 1:\n",
    "            BJpv = beta.cdf(spv, ii, N - ii + 1)\n",
    "            Mplus = np.min(BJpv)\n",
    "            Mminus = np.min(1 - BJpv)\n",
    "            bj = np.minimum(Mplus, Mminus)\n",
    "\n",
    "        return -np.log(np.maximum(bj, MIN_PVAL))\n",
    "\n",
    "    def hc_star(self, gamma=0.2):\n",
    "        \"\"\"sample-adjusted higher criticism score\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        'gamma' : lower fraction of P-values to consider\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        :HC_score:\n",
    "        :P-value attaining it:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        imin = self._imin_star\n",
    "        imax = np.maximum(imin + 1,\n",
    "                          int(np.floor(gamma * self._N + 0.5)))\n",
    "        return self._calculate_hc(imin, imax)\n",
    "\n",
    "    def hc_dashboard(self, gamma=0.2):\n",
    "        \"\"\"\n",
    "        Illustrates HC over z-scores and sorted P-values.\n",
    "\n",
    "        Args:\n",
    "            gamma:  HC parameter\n",
    "\n",
    "        Returns:\n",
    "            fig: an illustration of HC value\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        hc, hct = self.hc(gamma)\n",
    "        imin = 0\n",
    "        N = self._N\n",
    "        istar = self._istar\n",
    "        imax = np.maximum(imin, int(gamma * N + 0.5))\n",
    "\n",
    "        yy = np.sort(self._pvals)[imin:imax]\n",
    "        zz = self._zz[imin:imax]\n",
    "        xx = self._uu[imin:imax]\n",
    "\n",
    "        ax = plt.subplot(211)\n",
    "\n",
    "        ax.stem(xx, yy, markerfmt='.')\n",
    "        ax.plot([(istar + 1) / N, (istar + 1) / N], [0, hct], '--r', alpha=.75)\n",
    "        ax.set_ylabel('p-value', fontsize=14)\n",
    "        ax.set_title('Sorted P-values')\n",
    "        ax.set_xlim([0, imax / N])\n",
    "        ax.set_xlabel('i/n', fontsize=16)\n",
    "\n",
    "        labels = ax.get_xticklabels()\n",
    "        labels[-1].set_text(r\"$\\gamma_0=$\" + labels[-1]._text)\n",
    "        ax.set_xticks(ticks=[l._x for l in labels], labels=labels)\n",
    "\n",
    "        # second plot\n",
    "        ax = plt.subplot(212)\n",
    "        ax.plot(xx, zz)\n",
    "        ymin = np.min(zz) * 1.1\n",
    "        ax.plot([(istar + 1) / N, (istar + 1) / N], [ymin, hc], '--r', alpha=.75)\n",
    "\n",
    "        ax.plot([ymin, (istar + 1) / N], [hc, hc], '--r', alpha=.75)\n",
    "        ax.text(-0.01, hc, r'$HC$', horizontalalignment='center', fontsize=14,\n",
    "                bbox=dict(boxstyle=\"round\",\n",
    "                          ec=(1., 1, 1),\n",
    "                          fc=(1., 1, 1),\n",
    "                          alpha=0.5,\n",
    "                          ))\n",
    "\n",
    "        ax.set_ylabel('z-score', fontsize=14)\n",
    "\n",
    "        ax.grid(True)\n",
    "        ax.set_xlim([0, imax / N])\n",
    "        ax.set_xlabel('i/n', fontsize=16)\n",
    "\n",
    "        label = ax.get_xticklabels()[-1]\n",
    "        label.set_text(r\"$\\gamma_0=$\" + label._text)\n",
    "        ax.set_xticks(ticks=[label._x, (istar + 1) / N], labels=[label, str(np.round((istar + 1) / N, 2))])\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(10, 10, forward=True)\n",
    "\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def get_state(self):\n",
    "        return {'pvals': self._pvals,\n",
    "                'u': self._uu,\n",
    "                'z': self._zz,\n",
    "                'imin_star': self._imin_star,\n",
    "                'imin_jin': self._imin_jin,\n",
    "                }\n",
    "\n",
    "    def minp(self):\n",
    "        \"\"\"\n",
    "        Bonferroni type inference\n",
    "\n",
    "        -log(minimal P-value)\n",
    "        \"\"\"\n",
    "        return -np.log(np.maximum(self._pvals[0], MIN_PVAL))\n",
    "\n",
    "    def fdr(self):\n",
    "        \"\"\"\n",
    "        Maximal False-discovery rate functional \n",
    "\n",
    "        Returns:\n",
    "            :corrected critical P-value:\n",
    "            :critical P-value:\n",
    "        \"\"\"\n",
    "\n",
    "        vals = self._pvals / self._uu\n",
    "        istar = np.argmin(vals)\n",
    "        return -np.log(np.maximum(vals[istar], MIN_PVAL)), self._pvals[istar]\n",
    "\n",
    "    def fisher(self):\n",
    "        \"\"\"\n",
    "        combine P-values using Fisher's method:\n",
    "\n",
    "        Fs = sum(-2 log(pvals))\n",
    "\n",
    "        (here n is the number of P-values)\n",
    "\n",
    "        When pvals are uniform Fs ~ chi^2 with 2*len(pvals) degrees of freedom\n",
    "\n",
    "        Returns:\n",
    "            Fs:       Fisher's method statistics\n",
    "            Fs_pval:  P-value of the assocaited chi-squared test\n",
    "        \"\"\"\n",
    "\n",
    "        Fs = np.sum(-2 * np.log(np.maximum(self._pvals, MIN_PVAL)))\n",
    "        chi2_pval = chi2.sf(Fs, df=2 * len(self._pvals))\n",
    "        return Fs, chi2_pval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
