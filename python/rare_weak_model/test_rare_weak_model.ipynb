{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLG_lYgZWhQ-"
   },
   "source": [
    "### Porting to Google Colab\n",
    "The following cell enables this notebook to run from Google Colab as well as from your local machine IDE.<br>\n",
    "You can change `root_directory` and/or `this_notebook_google_path` to point to the directory in your Google account, which contains this notebook, together with the `imgs` sub-directory and the rest of the files.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22591,
     "status": "ok",
     "timestamp": 1735219246544,
     "user": {
      "displayName": "Dror Meirovich",
      "userId": "14310987058477987674"
     },
     "user_tz": -120
    },
    "id": "RwolyPfQWhQ_",
    "outputId": "d8fde0e6-ad07-4d03-ba4d-284f57ad778d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\projects\\RUNI\\Thesis\n",
      "Datasets path: d:\\projects\\RUNI\\Thesis\\datasets\n",
      "Output path: d:\\projects\\RUNI\\Thesis\\output\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive as google_drive # type: ignore\n",
    "except:\n",
    "    # no Google Colab --> fall back to local machine\n",
    "    google_drive = None\n",
    "\n",
    "if google_drive is not None:\n",
    "    google_drive_directory = os.path.join('/','content','gdrive')\n",
    "    google_drive.mount(google_drive_directory)\n",
    "    all_projects_path = os.path.join(google_drive_directory, 'Othercomputers','My Laptop', 'projects')\n",
    "else:\n",
    "    all_projects_path = os.path.join('d:\\\\', 'projects')\n",
    "\n",
    "project_path = os.path.join(all_projects_path,'RUNI','Thesis')\n",
    "assert os.path.exists(project_path), f'Project path {project_path} not found!'\n",
    "# enable import python files from this notebook's path\n",
    "sys.path.append(project_path)\n",
    "# enable reading images and data files from this notebook's path\n",
    "os.chdir(project_path)\n",
    "\n",
    "datasets_path = os.path.join(project_path, 'datasets')\n",
    "assert os.path.exists(datasets_path), f'Datasets path {datasets_path} not found!'\n",
    "\n",
    "output_path = os.path.join(project_path, 'output')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "assert os.path.exists(output_path), f'Output path {output_path} not found!'\n",
    "\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "print(f'Datasets path: {datasets_path}')\n",
    "print(f'Output path: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1735219246544,
     "user": {
      "displayName": "Dror Meirovich",
      "userId": "14310987058477987674"
     },
     "user_tz": -120
    },
    "id": "NaepIt5GH4HC"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7627,
     "status": "ok",
     "timestamp": 1735219254169,
     "user": {
      "displayName": "Dror Meirovich",
      "userId": "14310987058477987674"
     },
     "user_tz": -120
    },
    "id": "hs9MC9FbWhRB",
    "outputId": "714ecd47-ff73-49d1-a1bc-758274e96703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba version: 0.60.0\n",
      "numba.njit is available.\n",
      "CUDA is available and will be used for GPU operations.\n",
      "Printing CUDA active device attributes:\n",
      "==================================================\n",
      "    Name:                               NVIDIA GeForce GTX 1650\n",
      "    Free Memory:                        3367680 [KB]\n",
      "    Total Memory:                       4193984 [KB]\n",
      "    Compute capability:                 7.5\n",
      "    Clock rate:                         1560.00 [MHz]\n",
      "    Memory clock rate:                  4001.00 [MHz]\n",
      "    Memory bus width:                   128 bits\n",
      "    Memory band width (theoretical)     128.03 [GByte/Sec]\n",
      "    Number of multiprocessors:          16\n",
      "    Minimal grid size:                  128\n",
      "    Maximum grid size:                  (2147483647, 65535, 65535)\n",
      "    Maximum block dimensions:           (1024, 1024, 64)\n",
      "    Maximum threads per block:          1024\n",
      "    Warp size:                          32\n",
      "    Maximum shared memory per block:    49152 [bytes]\n",
      "    Maximum registers per block:        65536\n",
      "    Total constant memory:              65536 [bytes]\n",
      "    Asynchronous engine count:          2\n",
      "    L2 cache size:                      1048576 [bytes]\n",
      "    ECC support enabled:                False\n"
     ]
    }
   ],
   "source": [
    "from python.hpc import HybridArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEQE37mLH4HD"
   },
   "source": [
    "# Testing random p-values in vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6514,
     "status": "ok",
     "timestamp": 1735219260679,
     "user": {
      "displayName": "Dror Meirovich",
      "userId": "14310987058477987674"
     },
     "user_tz": -120
    },
    "id": "0JfT2FGNH4HD",
    "outputId": "efc32992-28fc-4625-a69a-a20dcac9ebaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_py.numpy().mean()=0.5002764624347839\n",
      "data_njit.numpy().mean()=0.5002764624347839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\thesis\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_gpu.numpy().mean()=0.5002764624347839\n"
     ]
    }
   ],
   "source": [
    "from python.rare_weak_model.rare_weak_model import random_p_values_series\n",
    "num_p_values = 100000\n",
    "seed = 3\n",
    "data_py = HybridArray()\n",
    "data_py.realloc(shape=(num_p_values,), dtype=np.float64, use_gpu=False)\n",
    "random_p_values_series(p_values_output=data_py, seed=seed, use_njit=False)\n",
    "print(f'{data_py.numpy().mean()=}')\n",
    "\n",
    "\n",
    "data_njit = HybridArray()\n",
    "data_njit.realloc(shape=(num_p_values,), dtype=np.float64, use_gpu=False)\n",
    "random_p_values_series(p_values_output=data_njit, seed=seed, use_njit=True)\n",
    "print(f'{data_njit.numpy().mean()=}')\n",
    "\n",
    "data_gpu = HybridArray()\n",
    "data_gpu.realloc(shape=(num_p_values,), dtype=np.float64, use_gpu=True)\n",
    "random_p_values_series(p_values_output=data_gpu, seed=seed)\n",
    "print(f'{data_gpu.numpy().mean()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12852,
     "status": "ok",
     "timestamp": 1735219273529,
     "user": {
      "displayName": "Dror Meirovich",
      "userId": "14310987058477987674"
     },
     "user_tz": -120
    },
    "id": "Jww4YDXcH4HD",
    "outputId": "4767ffd3-5b2b-42dd-9eb2-770108006d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_py.numpy()=array([[0.5026821 , 0.54580257, 0.96072045, 0.10704762, 0.85635647],\n",
      "       [0.81462697, 0.12963367, 0.67156386, 0.98788386, 0.32445072],\n",
      "       [0.63443802, 0.24619511, 0.02131631, 0.59151714, 0.87104361],\n",
      "       [0.13928325, 0.25919254, 0.84848738, 0.71313253, 0.61695041],\n",
      "       [0.64173835, 0.92926785, 0.52276469, 0.12200263, 0.05807142]])\n",
      "data_njit.numpy()=array([[0.5026821 , 0.54580257, 0.96072045, 0.10704762, 0.85635647],\n",
      "       [0.81462697, 0.12963367, 0.67156386, 0.98788386, 0.32445072],\n",
      "       [0.63443802, 0.24619511, 0.02131631, 0.59151714, 0.87104361],\n",
      "       [0.13928325, 0.25919254, 0.84848738, 0.71313253, 0.61695041],\n",
      "       [0.64173835, 0.92926785, 0.52276469, 0.12200263, 0.05807142]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\thesis\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_gpu.numpy()=array([[0.5026821 , 0.54580257, 0.96072045, 0.10704762, 0.85635647],\n",
      "       [0.81462697, 0.12963367, 0.67156386, 0.98788386, 0.32445072],\n",
      "       [0.63443802, 0.24619511, 0.02131631, 0.59151714, 0.87104361],\n",
      "       [0.13928325, 0.25919254, 0.84848738, 0.71313253, 0.61695041],\n",
      "       [0.64173835, 0.92926785, 0.52276469, 0.12200263, 0.05807142]])\n"
     ]
    }
   ],
   "source": [
    "from python.rare_weak_model.rare_weak_model import random_p_values_matrix\n",
    "shape = (5,5)\n",
    "seed = 0\n",
    "data_py = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=False)\n",
    "num_steps = 1\n",
    "num_steps = 10\n",
    "random_p_values_matrix(p_values_output=data_py, offset_row0=seed, offset_col0=0, num_steps=num_steps, use_njit=False)\n",
    "print(f'{data_py.numpy()=}')\n",
    "\n",
    "\n",
    "data_njit = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=False)\n",
    "random_p_values_matrix(p_values_output=data_njit, offset_row0=seed, offset_col0=0, num_steps=num_steps, use_njit=True)\n",
    "print(f'{data_njit.numpy()=}')\n",
    "\n",
    "data_gpu = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=True)\n",
    "random_p_values_matrix(p_values_output=data_gpu, offset_row0=seed, offset_col0=0, num_steps=num_steps)\n",
    "print(f'{data_gpu.numpy()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3735,
     "status": "ok",
     "timestamp": 1735219277262,
     "user": {
      "displayName": "Dror Meirovich",
      "userId": "14310987058477987674"
     },
     "user_tz": -120
    },
    "id": "XIOazHvDH4HE",
    "outputId": "665a3683-75c1-4358-f9dc-69a9e95ef0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_py.numpy()=array([[0.16028751, 0.18809541, 0.77610614, 0.01246831, 0.52555148],\n",
      "       [0.45821836, 0.01666338, 0.28918595, 0.8949747 , 0.07279465],\n",
      "       [0.2557933 , 0.0458486 , 0.00123378, 0.22108006, 0.5522461 ],\n",
      "       [0.01860078, 0.04989869, 0.51195353, 0.33089593, 0.24117783],\n",
      "       [0.26209799, 0.68095106, 0.1728644 , 0.01519254, 0.00506776]])\n",
      "data_njit.numpy()=array([[0.16028751, 0.18809541, 0.77610614, 0.01246831, 0.52555148],\n",
      "       [0.45821836, 0.01666338, 0.28918595, 0.8949747 , 0.07279465],\n",
      "       [0.2557933 , 0.0458486 , 0.00123378, 0.22108006, 0.5522461 ],\n",
      "       [0.01860078, 0.04989869, 0.51195353, 0.33089593, 0.24117783],\n",
      "       [0.26209799, 0.68095106, 0.1728644 , 0.01519254, 0.00506776]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\thesis\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_gpu.numpy()=array([[0.16028751, 0.18809541, 0.77610614, 0.01246831, 0.52555148],\n",
      "       [0.45821836, 0.01666338, 0.28918595, 0.8949747 , 0.07279465],\n",
      "       [0.2557933 , 0.0458486 , 0.00123378, 0.22108006, 0.5522461 ],\n",
      "       [0.01860078, 0.04989869, 0.51195353, 0.33089593, 0.24117783],\n",
      "       [0.26209799, 0.68095106, 0.1728644 , 0.01519254, 0.00506776]])\n"
     ]
    }
   ],
   "source": [
    "from python.rare_weak_model.rare_weak_model import random_modified_p_values_matrix\n",
    "shape = (5,5)\n",
    "seed = 0\n",
    "data_py = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=False)\n",
    "num_steps = 1\n",
    "num_steps = 10\n",
    "mu = 1\n",
    "random_modified_p_values_matrix(p_values_output=data_py, mu=mu, offset_row0=seed, offset_col0=0, num_steps=num_steps, use_njit=False)\n",
    "print(f'{data_py.numpy()=}')\n",
    "\n",
    "\n",
    "data_njit = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=False)\n",
    "random_modified_p_values_matrix(p_values_output=data_njit, mu=mu, offset_row0=seed, offset_col0=0, num_steps=num_steps, use_njit=True)\n",
    "print(f'{data_njit.numpy()=}')\n",
    "\n",
    "data_gpu = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=True)\n",
    "random_modified_p_values_matrix(p_values_output=data_gpu, mu=mu, offset_row0=seed, offset_col0=0, num_steps=num_steps)\n",
    "print(f'{data_gpu.numpy()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values N=5 n1=2\n",
      "[[0.05673545 0.16622495 0.23860856 0.39280115 0.59610853]\n",
      " [0.03518283 0.12296948 0.43450244 0.49564988 0.49807309]\n",
      " [0.21532803 0.28434879 0.36328378 0.46521514 0.85837417]\n",
      " [0.12733791 0.31513581 0.42174821 0.61900583 0.81407409]\n",
      " [0.03489916 0.19571534 0.20613945 0.2407543  0.98123859]]\n",
      "\n",
      "Native Python:\n",
      "data=\n",
      "[[0.00596535 0.03525797 0.03608516 0.28501832 0.44151926]\n",
      " [0.0130165  0.21395014 0.2997102  0.62444604 0.62525435]\n",
      " [0.28240602 0.6765417  0.7014569  0.80739647 0.90617889]\n",
      " [0.14628628 0.22627942 0.33122183 0.52415429 0.56540923]\n",
      " [0.01185262 0.04504833 0.43827133 0.56865378 0.94735358]]\n",
      "counts=\n",
      "[[0 1 1 2 2]\n",
      " [1 1 1 2 2]\n",
      " [0 1 1 1 2]\n",
      " [0 1 1 2 2]\n",
      " [1 2 2 2 2]]\n",
      "\n",
      "Numba NJIT\n",
      "data=\n",
      "[[0.00596535 0.03525797 0.03608516 0.28501832 0.44151926]\n",
      " [0.0130165  0.21395014 0.2997102  0.62444604 0.62525435]\n",
      " [0.28240602 0.6765417  0.7014569  0.80739647 0.90617889]\n",
      " [0.14628628 0.22627942 0.33122183 0.52415429 0.56540923]\n",
      " [0.01185262 0.04504833 0.43827133 0.56865378 0.94735358]]\n",
      "counts=\n",
      "[[0 1 1 2 2]\n",
      " [1 1 1 2 2]\n",
      " [0 1 1 1 2]\n",
      " [0 1 1 2 2]\n",
      " [1 2 2 2 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\thesis\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numba CUDA\n",
      "data=\n",
      "[[0.00596535 0.03525797 0.03608516 0.28501832 0.44151926]\n",
      " [0.0130165  0.21395014 0.2997102  0.62444604 0.62525435]\n",
      " [0.28240602 0.6765417  0.7014569  0.80739647 0.90617889]\n",
      " [0.14628628 0.22627942 0.33122183 0.52415429 0.56540923]\n",
      " [0.01185262 0.04504833 0.43827133 0.56865378 0.94735358]]\n",
      "counts=\n",
      "[[0 1 1 2 2]\n",
      " [1 1 1 2 2]\n",
      " [0 1 1 1 2]\n",
      " [0 1 1 2 2]\n",
      " [1 2 2 2 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv\\thesis\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from python.rare_weak_model.rare_weak_model import rare_weak_model\n",
    "shape = (5,5)\n",
    "n1 = 2\n",
    "seed = 0\n",
    "data_py = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=False)\n",
    "counts_py = HybridArray()\n",
    "num_steps = 1\n",
    "mu = 1\n",
    "rare_weak_model(sorted_p_values_output=data_py, cumulative_counts_output=counts_py, mu=mu, n1=n1, num_steps=num_steps, use_njit=False, sort_labels=False)\n",
    "print(f'Original values N={shape[1]} {n1=}\\n{data_py.numpy()}')\n",
    "\n",
    "\n",
    "rare_weak_model(sorted_p_values_output=data_py, cumulative_counts_output=counts_py, mu=mu, n1=n1, use_njit=False)\n",
    "print('\\nNative Python:')\n",
    "print(f'data=\\n{data_py.numpy()}')\n",
    "print(f'counts=\\n{counts_py.numpy()}')\n",
    "\n",
    "data_njit = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=False)\n",
    "counts_njit = HybridArray()\n",
    "rare_weak_model(sorted_p_values_output=data_njit, cumulative_counts_output=counts_njit, mu=mu, n1=n1, use_njit=True)\n",
    "print('\\nNumba NJIT')\n",
    "print(f'data=\\n{data_njit.numpy()}')\n",
    "print(f'counts=\\n{counts_njit.numpy()}')\n",
    "\n",
    "data_gpu = HybridArray().realloc(shape=shape, dtype=np.float64, use_gpu=True)\n",
    "counts_gpu = HybridArray()\n",
    "rare_weak_model(sorted_p_values_output=data_gpu, cumulative_counts_output=counts_gpu, mu=mu, n1=n1)\n",
    "print('\\nNumba CUDA')\n",
    "print(f'data=\\n{data_gpu.numpy()}')\n",
    "print(f'counts=\\n{counts_gpu.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
